{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Teaching a Model to Write using KerasNLP - An Introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "1tASY7ohdoZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIZw4idT8G2m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import regex as re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data for Learning\n",
        "\n",
        "We can retrieve data from a webpage to teach our model to write jokes."
      ],
      "metadata": {
        "id": "jqo48fF0d3DP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXV4gjzgAT6k"
      },
      "outputs": [],
      "source": [
        "# Reading from a page with the required libraries\n",
        "url = \"https://einfachreisenmitkind.de/egal-wie-witze-sprueche/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "jokes = []\n",
        "for joke in soup.find_all(\"p\"):\n",
        "    if joke.text.startswith(\"Egal wie\"):\n",
        "        jokes.append(joke.text.strip())\n",
        "\n",
        "with open(\"jokes.txt\", \"w\") as f:\n",
        "    for joke in jokes:\n",
        "        f.write(joke + \"\\n\")\n",
        "\n",
        "print(f\"There are {len(jokes)} jokes. An example is '{jokes[-1]}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFI8ejZ08QA4"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "def file_to_sentence_list(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Splitting the text into sentences using\n",
        "    # delimiters like '.', '?', and '!'\n",
        "    sentences = [sentence.strip() for sentence in re.split(\n",
        "        r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "file_path = './jokes.txt'\n",
        "text_data = file_to_sentence_list(file_path)\n",
        "\n",
        "print(f\"There are {len(text_data)} lines in this dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Below, you create an instance of the `Tokenizer` class provided by the Keras library. The Tokenizer class is used for tokenizing text, which involves breaking down a sequence of text into individual words or subwords.\n",
        "\n",
        "The `fit_on_texts` method is used to update the internal state of the tokenizer based on the provided text data (text_data). This involves building the vocabulary (word_index) and updating various internal structures to facilitate the tokenization process.\n",
        "\n",
        "The `text_data` is expected to be a list of texts (or a single text string). Each text is a sequence of words that the tokenizer will process to create a vocabulary."
      ],
      "metadata": {
        "id": "k4ANml7ifDpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "8uoXO3r655pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will discuss the code responsible for creating input sequences that will be used to train the neural network. Let's break it down step by step:\n",
        "\n",
        "`input_sequences` is a list that will store sequences of words.\n",
        "The loop iterates over each line in `text_data`, which is assumed to be a list of texts or a single text string.\n",
        "\n",
        "For each line, it uses `tokenizer.texts_to_sequences([line])` to convert the text into a sequence of word indices. The [0] at the end is used to extract the list of indices from the result, as texts_to_sequences returns a list of lists.\n",
        "\n",
        "The inner loop iterates over the indices in `token_list` starting from index 1. For each index i, it creates an n-gram sequence (n_gram_sequence) by taking the subsequence from the beginning of token_list up to index i+1. This represents a sequence of words from the start to the current position in the line.\n",
        "\n",
        "The `n_gram_sequence` is then appended to the input_sequences list."
      ],
      "metadata": {
        "id": "UnU0C5nDgJeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in text_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for uniform length\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "print(f\"Total words in the dataset: {total_words}\")"
      ],
      "metadata": {
        "id": "AtlAn1vDfnli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = np.eye(total_words)[y]  # One-hot encode the labels"
      ],
      "metadata": {
        "id": "es_AC2Ks5_ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition and Training\n",
        "\n",
        "*Initialize a Sequential Model:*\n",
        "\n",
        "This line creates a sequential model, which is a linear stack of layers. It's a simple way to build a model where each layer has exactly one input tensor and one output tensor.\n",
        "\n",
        "*Add an Embedding Layer:*\n",
        "- The Embedding layer is used for word embedding. It transforms positive integers (word indices) into dense vectors of fixed size.\n",
        "- total_words is the input dimension, representing the total number of unique words in the vocabulary.\n",
        "- 50 is the output dimension, meaning each word will be represented as a 50-dimensional vector.\n",
        "- input_length is set to max_sequence_length-1, which is the length of the input sequences minus one. It defines the size of each input sequence the model will receive.\n",
        "\n",
        "*Add an LSTM Layer:*\n",
        "- The LSTM (Long Short-Term Memory) layer is a type of recurrent neural network (RNN) layer. It is particularly effective for sequence data.\n",
        "- 100 is the number of LSTM units or cells in the layer. This parameter controls the complexity and capacity of the LSTM layer.\n",
        "\n",
        "*Add a Dense Output Layer:*\n",
        "- The Dense layer is a fully connected layer that produces the output of the neural network.\n",
        "- total_words is the number of units in the output layer, representing the total number of unique words in the vocabulary.\n",
        "- activation='softmax' applies the softmax activation function, which is common for multi-class classification problems. It converts the raw output scores into probabilities.\n",
        "\n",
        "*Compile the Model:*\n",
        "- Compiles the model, specifying the optimizer, loss function, and evaluation metric(s).\n",
        "- optimizer='adam' sets the Adam optimization algorithm, which is widely used in deep learning.\n",
        "- loss='categorical_crossentropy' is the loss function used for multi-class classification problems.\n",
        "- metrics=['accuracy'] specifies that the accuracy should be monitored during training."
      ],
      "metadata": {
        "id": "NoHD8X0phDVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "t8B-r45V6gWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "_JVz9VX77r4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferring on Trained Model\n",
        "\n",
        "We have now completed training the model with the data that is available. Let us look at how to generate new texts using the model that we completed training.\n",
        "\n",
        "- seed_text is the starting point for generating text. It serves as the initial input to the model, and the goal is to predict the next words based on this seed text.\n",
        "- The loop runs for 10 iterations, generating 10 words (or tokens) one at a time.\n",
        "- tokenizer.texts_to_sequences([seed_text]) converts the seed text into a sequence of word indices using the same tokenizer that was used during training.\n",
        "- pad_sequences pads the sequence to have the same length (max_sequence_length-1) as the input sequences used during training. Padding is applied to the beginning of the sequence (padding='pre').\n",
        "\n",
        "- model.predict(token_list) uses the trained model to predict the next word in the sequence based on the input token_list.\n",
        "- np.argmax is used to find the index of the word with the highest predicted probability.\n",
        "- tokenizer.index_word[predicted_word_index[0]] converts the predicted index back into the actual word using the inverse mapping from the tokenizer."
      ],
      "metadata": {
        "id": "RIKbaJ4Riqgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Langsam ging der Richter von Fenster\"\n",
        "for _ in range(10):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted_word_index = np.argmax(model.predict(token_list), axis=-1)\n",
        "    predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "    seed_text += \" \" + predicted_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "65zDOivG7x6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Trained Model and Tokenizer"
      ],
      "metadata": {
        "id": "BrEve1qclbo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open('model_tokenizer_jokes.pickle', 'wb') as handle:\n",
        "    pkl.dump([model, tokenizer], handle, protocol=pkl.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "9eILMekbA_NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Trained Model and Tokenizer"
      ],
      "metadata": {
        "id": "WmPJtpY2lsHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer used during training\n",
        "with open('./model_tokenizer_jokes.pickle', 'rb') as file:\n",
        "    [model_1, tokenizer_1] = pkl.load(file)"
      ],
      "metadata": {
        "id": "f3EGeD3jBunO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get an idea for the trained model and tokenizer sizes"
      ],
      "metadata": {
        "id": "2D_VmhDLlvRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lrt"
      ],
      "metadata": {
        "id": "kwNQU0VQB33q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer on Loaded Model\n",
        "\n",
        "Check on the outputs produced by the two seed texts below. Compare this to the outputs produced by the model trained with larger text.\n",
        "\n",
        "1. Does the model trained on larger text provide better results?\n",
        "2. Which model produces better text?\n",
        "3. Try with new seed texts to understand the shortcomings of the trained model."
      ],
      "metadata": {
        "id": "3Ms_G7Y9l1Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input seed text\n",
        "seed_text = \"Langsam ging der Richter von Fenster\"\n",
        "token_list = tokenizer_1.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "\n",
        "# Use the model to predict the next word\n",
        "predicted_word_index = np.argmax(model_1.predict(token_list, verbose=0), axis=-1)\n",
        "predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "\n",
        "print(f\"Predicted sentence: {seed_text} {predicted_word}\")"
      ],
      "metadata": {
        "id": "zJqrwiaSB9TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input seed text\n",
        "seed_text = \"Egal wie hart du bist, sie sind\"\n",
        "token_list = tokenizer_1.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "\n",
        "# Use the model to predict the next word\n",
        "predicted_word_index = np.argmax(model_1.predict(token_list, verbose=0), axis=-1)\n",
        "predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "\n",
        "print(f\"Predicted sentence: {seed_text} {predicted_word}\")"
      ],
      "metadata": {
        "id": "1XpgaXMLCY2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}