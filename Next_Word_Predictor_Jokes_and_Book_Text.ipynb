{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Teaching a Model to Write using KerasNLP - An Introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "h7OmO2qUdEXl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIZw4idT8G2m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb-uFofTXK2y"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/computationalstylistics/68_german_novels/master/corpus/achleitner_bergrichters.txt -O text1.txt\n",
        "# !wget https://raw.githubusercontent.com/computationalstylistics/68_german_novels/master/corpus/achleitner_celsissimus.txt -O text2.txt\n",
        "# !wget https://raw.githubusercontent.com/computationalstylistics/68_german_novels/master/corpus/achleitner_tann.txt -O text3.txt\n",
        "# !wget https://raw.githubusercontent.com/computationalstylistics/68_german_novels/master/corpus/anonym_schwester.txt -O text4.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXV4gjzgAT6k"
      },
      "outputs": [],
      "source": [
        "url = \"https://einfachreisenmitkind.de/egal-wie-witze-sprueche/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# print(soup.find_all(\"p\"))\n",
        "jokes = []\n",
        "for joke in soup.find_all(\"p\"):\n",
        "    if joke.text.startswith(\"Egal wie\"):\n",
        "        jokes.append(joke.text.strip())\n",
        "\n",
        "with open(\"jokes.txt\", \"w\") as f:\n",
        "    for joke in jokes:\n",
        "        f.write(joke + \"\\n\")\n",
        "\n",
        "print(f\"There are {len(jokes)} jokes. An example is '{jokes[-1]}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2yLKT-hYbju"
      },
      "outputs": [],
      "source": [
        "!cat text1.txt jokes.txt > data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFI8ejZ08QA4"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "def file_to_sentence_list(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Splitting the text into sentences using\n",
        "    # delimiters like '.', '?', and '!'\n",
        "    sentences = [sentence.strip() for sentence in re.split(\n",
        "        r'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "file_path = './data.txt'\n",
        "text_data = file_to_sentence_list(file_path)\n",
        "\n",
        "print(len(text_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in text_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for uniform length\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "print(f\"Total words in the dataset: {total_words}\")"
      ],
      "metadata": {
        "id": "8uoXO3r655pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = np.eye(total_words)[y]  # One-hot encode the labels"
      ],
      "metadata": {
        "id": "es_AC2Ks5_ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "t8B-r45V6gWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "3eZt6JTn6kF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "_JVz9VX77r4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Langsam ging der Richter von Fenster\"\n",
        "for _ in range(10):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted_word_index = np.argmax(model.predict(token_list), axis=-1)\n",
        "    predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "    seed_text += \" \" + predicted_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "65zDOivG7x6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open('model_tokenizer_text_jokes.pickle', 'wb') as handle:\n",
        "    pkl.dump([model, tokenizer], handle, protocol=pkl.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "9eILMekbA_NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer used during training\n",
        "with open('./model_tokenizer_text_jokes.pickle', 'rb') as file:\n",
        "    [model_1, tokenizer_1] = pkl.load(file)"
      ],
      "metadata": {
        "id": "f3EGeD3jBunO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lrt"
      ],
      "metadata": {
        "id": "kwNQU0VQB33q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input seed text\n",
        "seed_text = \"Langsam ging der Richter von Fenster\"\n",
        "token_list = tokenizer_1.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "\n",
        "# Use the model to predict the next word\n",
        "predicted_word_index = np.argmax(model_1.predict(token_list, verbose=0), axis=-1)\n",
        "predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "\n",
        "print(f\"Predicted sentence: {seed_text} {predicted_word}\")"
      ],
      "metadata": {
        "id": "zJqrwiaSB9TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input seed text\n",
        "seed_text = \"Egal wie hart du bist, sie sind\"\n",
        "token_list = tokenizer_1.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "\n",
        "# Use the model to predict the next word\n",
        "predicted_word_index = np.argmax(model_1.predict(token_list, verbose=0), axis=-1)\n",
        "predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "\n",
        "print(f\"Predicted sentence: {seed_text} {predicted_word}\")"
      ],
      "metadata": {
        "id": "1XpgaXMLCY2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}